{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SlyFox579/bdt_postblocks/blob/main/Copy_of_pyspark_postgres_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Purpose\n",
        "\n",
        "Explore PySpark and the JDBC connection functionality to read from operational databases.\n",
        "\n",
        "In this notebook we will setup a PostgreSQL instance and populate it with the Pagila dataset. We will then connect to the database via a JDBC connector."
      ],
      "metadata": {
        "id": "8QbPsmEf6ljt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "\n",
        "## PostgreSQL\n",
        "\n",
        "Firstly, let's install postgres in the this Colab instance."
      ],
      "metadata": {
        "id": "f-RHL4bg4u0_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install postgresql postgresql-contrib "
      ],
      "metadata": {
        "id": "qhmGVh22JcNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!service postgresql start"
      ],
      "metadata": {
        "id": "ajhL0Z_-KK8r",
        "outputId": "d9610e6f-2a52-4687-dd5e-354de4ff6dcb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Starting PostgreSQL 10 database server\n",
            "   ...done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a user in Postgres ([stackoverflow](https://stackoverflow.com/questions/12720967/how-to-change-postgresql-user-password/12721020#12721020))\n"
      ],
      "metadata": {
        "id": "_P48P8Vt6Fm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo -u postgres psql -c \"ALTER USER postgres PASSWORD 'test';\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b25UVuzVNdKs",
        "outputId": "79cadd36-c1a2-4b3d-b9d8-9983767acd69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ALTER ROLE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Store you database password in an environmental variable so that we need no type it in all the time (not advisable generally).\n",
        "\n",
        "We'll use the notebook magic `%end`"
      ],
      "metadata": {
        "id": "JW1kucySWAKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%env PGPASSWORD=test"
      ],
      "metadata": {
        "id": "as0Zs9kL6PY0",
        "outputId": "a644fbb3-73e6-4b63-9716-442edf647545",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: PGPASSWORD=test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pagila\n",
        "\n",
        "Now, let's populate the PostgreSQL database with the Pagila data from the tutorial."
      ],
      "metadata": {
        "id": "LGqYbg366efu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/spatialedge-ai/pagila.git"
      ],
      "metadata": {
        "id": "qICjoP_dKS8G",
        "outputId": "50441d8e-8f10-4fcd-b992-46257e11211c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pagila'...\n",
            "remote: Enumerating objects: 94, done.\u001b[K\n",
            "remote: Counting objects: 100% (94/94), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 94 (delta 46), reused 87 (delta 42), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (94/94), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!psql -h localhost -U postgres -c \"create database pagila\""
      ],
      "metadata": {
        "id": "xYHVKYqSMthy",
        "outputId": "1286d367-387f-49ab-a018-a1aedf21ea90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CREATE DATABASE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!psql -h localhost -U postgres -d pagila -f \"pagila/pagila-schema.sql\""
      ],
      "metadata": {
        "id": "kfgNogz3MSq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!psql -h localhost -U postgres -d pagila -f \"pagila/pagila-data.sql\""
      ],
      "metadata": {
        "id": "8zpqaYNZPABo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PySpark Setup\n",
        "\n",
        "Now, let's download what is necessary for initiating jdbc connections, as well as what is required to run PySpark itself."
      ],
      "metadata": {
        "id": "9M0a4GiI6yyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://stackoverflow.com/questions/34948296/using-pyspark-to-connect-to-postgresql\n",
        "!wget https://jdbc.postgresql.org/download/postgresql-42.5.0.jar"
      ],
      "metadata": {
        "id": "bCiCzTg-Jx2Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a864197-4502-4957-f194-15c7b4a1ace7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-09-25 20:15:18--  https://jdbc.postgresql.org/download/postgresql-42.5.0.jar\n",
            "Resolving jdbc.postgresql.org (jdbc.postgresql.org)... 72.32.157.228, 2001:4800:3e1:1::228\n",
            "Connecting to jdbc.postgresql.org (jdbc.postgresql.org)|72.32.157.228|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1046274 (1022K) [application/java-archive]\n",
            "Saving to: ‘postgresql-42.5.0.jar’\n",
            "\n",
            "postgresql-42.5.0.j 100%[===================>]   1022K  5.11MB/s    in 0.2s    \n",
            "\n",
            "2022-09-25 20:15:18 (5.11 MB/s) - ‘postgresql-42.5.0.jar’ saved [1046274/1046274]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np  \n",
        "\n",
        "%config Completer.use_jedi = False\n",
        "\n",
        "\n",
        "SPARKVERSION='2.4.8'\n",
        "HADOOPVERSION='2.7'\n",
        "pwd=os.getcwd()\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"{pwd}/spark-{SPARKVERSION}-bin-hadoop{HADOOPVERSION}\"\n",
        "\n",
        "print(os.environ['SPARK_HOME'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BQsxrwZBhWc",
        "outputId": "9b051665-d8d2-4b1a-85eb-849e1a562cba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/spark-2.4.8-bin-hadoop2.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget https://archive.apache.org/dist/spark/spark-{SPARKVERSION}/spark-{SPARKVERSION}-bin-hadoop{HADOOPVERSION}.tgz\n",
        "!tar xf spark-{SPARKVERSION}-bin-hadoop{HADOOPVERSION}.tgz"
      ],
      "metadata": {
        "id": "1owkTgHVBuix",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35ddf436-7bf5-459a-c1bb-75d2564b5ee5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "--2022-09-25 20:15:48--  https://archive.apache.org/dist/spark/spark-2.4.8/spark-2.4.8-bin-hadoop2.7.tgz\n",
            "Resolving archive.apache.org (archive.apache.org)... 138.201.131.134, 2a01:4f8:172:2ec5::2\n",
            "Connecting to archive.apache.org (archive.apache.org)|138.201.131.134|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 235899716 (225M) [application/x-gzip]\n",
            "Saving to: ‘spark-2.4.8-bin-hadoop2.7.tgz’\n",
            "\n",
            "spark-2.4.8-bin-had 100%[===================>] 224.97M  19.9MB/s    in 12s     \n",
            "\n",
            "2022-09-25 20:16:01 (18.2 MB/s) - ‘spark-2.4.8-bin-hadoop2.7.tgz’ saved [235899716/235899716]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp postgresql-42.5.0.jar spark-2.4.8-bin-hadoop2.7/jars"
      ],
      "metadata": {
        "id": "Ighjc_WdUNgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install findspark"
      ],
      "metadata": {
        "id": "gCIQhdSYC5uh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c626dd4c-b612-4015-92f6-71d35ef56cc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# get a spark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.config(\"spark.jars\", \n",
        "                                                       \"postgresql-42.2.5.jar\").config(\n",
        "                                                          \"spark.driver.extraClassPath\",\n",
        "                                                          \"spark-2.4.8-bin-hadoop2.7/jars\"\n",
        "                                                       ).getOrCreate()\n",
        "print(spark.conf.get('spark.jars'))\n",
        "\n",
        "%env PYARROW_IGNORE_TIMEZONE=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reMhwdxpCz05",
        "outputId": "71763211-a694-4199-f393-4b97723b6c04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "postgresql-42.2.5.jar\n",
            "env: PYARROW_IGNORE_TIMEZONE=1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Questions\n",
        "\n",
        "### Question 1\n",
        "\n",
        "Using a PySpark dataframe, print the schema of customer table in the pagila PostgreSQL database by utilising a JDBC connection."
      ],
      "metadata": {
        "id": "IqG_Hk4YXuC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pyspark code\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "       .appName(\"Python Spark SQL basic example\") \\\n",
        "       .config(\"spark.jars\", \"/content/postgresql-42.5.0.jar\") \\\n",
        "       .getOrCreate()\n",
        "\n",
        "iris_df = spark.read \\\n",
        "          .format(\"jdbc\") \\\n",
        "          .option(\"url\", \"jdbc:postgresql://localhost:5432/pagila\") \\\n",
        "    \t  .option(\"dbtable\", \"customer\") \\\n",
        "    \t  .option(\"user\", \"postgres\") \\\n",
        "    \t  .option(\"password\", \"test\") \\\n",
        "    \t  .option(\"driver\", \"org.postgresql.Driver\") \\\n",
        "    \t  .load()\n",
        "\n",
        "iris_df.show(10)"
      ],
      "metadata": {
        "id": "EnrQk09jQyaJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfa0c353-a4ec-470e-eba1-3b5ee185afc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------+----------+---------+--------------------+----------+----------+-----------+-------------------+------+\n",
            "|customer_id|store_id|first_name|last_name|               email|address_id|activebool|create_date|        last_update|active|\n",
            "+-----------+--------+----------+---------+--------------------+----------+----------+-----------+-------------------+------+\n",
            "|          1|       1|      MARY|    SMITH|MARY.SMITH@sakila...|         5|      true| 2020-02-14|2020-02-15 09:57:20|     1|\n",
            "|          2|       1|  PATRICIA|  JOHNSON|PATRICIA.JOHNSON@...|         6|      true| 2020-02-14|2020-02-15 09:57:20|     1|\n",
            "|          3|       1|     LINDA| WILLIAMS|LINDA.WILLIAMS@sa...|         7|      true| 2020-02-14|2020-02-15 09:57:20|     1|\n",
            "|          4|       2|   BARBARA|    JONES|BARBARA.JONES@sak...|         8|      true| 2020-02-14|2020-02-15 09:57:20|     1|\n",
            "|          5|       1| ELIZABETH|    BROWN|ELIZABETH.BROWN@s...|         9|      true| 2020-02-14|2020-02-15 09:57:20|     1|\n",
            "|          6|       2|  JENNIFER|    DAVIS|JENNIFER.DAVIS@sa...|        10|      true| 2020-02-14|2020-02-15 09:57:20|     1|\n",
            "|          7|       1|     MARIA|   MILLER|MARIA.MILLER@saki...|        11|      true| 2020-02-14|2020-02-15 09:57:20|     1|\n",
            "|          8|       2|     SUSAN|   WILSON|SUSAN.WILSON@saki...|        12|      true| 2020-02-14|2020-02-15 09:57:20|     1|\n",
            "|          9|       2|  MARGARET|    MOORE|MARGARET.MOORE@sa...|        13|      true| 2020-02-14|2020-02-15 09:57:20|     1|\n",
            "|         10|       1|   DOROTHY|   TAYLOR|DOROTHY.TAYLOR@sa...|        14|      true| 2020-02-14|2020-02-15 09:57:20|     1|\n",
            "+-----------+--------+----------+---------+--------------------+----------+----------+-----------+-------------------+------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 2\n",
        "\n",
        "Use the Spark SQL API to query the customer table, compute the number of unique email addresses in that table and print the result in the notebook."
      ],
      "metadata": {
        "id": "tXhnjaylCFI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pyspark code\n",
        "iris_df.registerTempTable(\"customer\")\n",
        "select = spark.sql(\"\"\"SELECT * FROM customer\"\"\")\n",
        "select.show(20)"
      ],
      "metadata": {
        "id": "xTGwAFhYpanl",
        "outputId": "35abffeb-3b62-40de-b45d-af7bce93b22f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------+----------+---------+--------------------+----------+----------+-----------+-------------------+------+\n",
            "|customer_id|store_id|first_name|last_name|               email|address_id|activebool|create_date|        last_update|active|\n",
            "+-----------+--------+----------+---------+--------------------+----------+----------+-----------+-------------------+------+\n",
            "|          1|       1|      MARY|    SMITH|MARY.SMITH@sakila...|         5|      true| 2020-02-14|2020-02-15 09:57:20|     1|\n",
            "|          2|       1|  PATRICIA|  JOHNSON|PATRICIA.JOHNSON@...|         6|      true| 2020-02-14|2020-02-15 09:57:20|     1|\n",
            "|          3|       1|     LINDA| WILLIAMS|LINDA.WILLIAMS@sa...|         7|      true| 2020-02-14|2020-02-15 09:57:20|     1|\n",
            "|          4|       2|   BARBARA|    JONES|BARBARA.JONES@sak...|         8|      true| 2020-02-14|2020-02-15 09:57:20|     1|\n",
            "|          5|       1| ELIZABETH|    BROWN|ELIZABETH.BROWN@s...|         9|      true| 2020-02-14|2020-02-15 09:57:20|     1|\n",
            "|          6|       2|  JENNIFER|    DAVIS|JENNIFER.DAVIS@sa...|        10|      true| 2020-02-14|2020-02-15 09:57:20|     1|\n",
            "|          7|       1|     MARIA|   MILLER|MARIA.MILLER@saki...|        11|      true| 2020-02-14|2020-02-15 09:57:20|     1|\n",
            "|          8|       2|     SUSAN|   WILSON|SUSAN.WILSON@saki...|        12|      true| 2020-02-14|2020-02-15 09:57:20|     1|\n",
            "|          9|       2|  MARGARET|    MOORE|MARGARET.MOORE@sa...|        13|      true| 2020-02-14|2020-02-15 09:57:20|     1|\n",
            "|         10|       1|   DOROTHY|   TAYLOR|DOROTHY.TAYLOR@sa...|        14|      true| 2020-02-14|2020-02-15 09:57:20|     1|\n",
            "|         11|       2|      LISA| ANDERSON|LISA.ANDERSON@sak...|        15|      true| 2020-02-14|2020-02-15 09:57:20|     1|\n",
            "|         12|       1|     NANCY|   THOMAS|NANCY.THOMAS@saki...|        16|      true| 2020-02-14|2020-02-15 09:57:20|     1|\n",
            "|         13|       2|     KAREN|  JACKSON|KAREN.JACKSON@sak...|        17|      true| 2020-02-14|2020-02-15 09:57:20|     1|\n",
            "|         14|       2|     BETTY|    WHITE|BETTY.WHITE@sakil...|        18|      true| 2020-02-14|2020-02-15 09:57:20|     1|\n",
            "|         15|       1|     HELEN|   HARRIS|HELEN.HARRIS@saki...|        19|      true| 2020-02-14|2020-02-15 09:57:20|     1|\n",
            "|         16|       2|    SANDRA|   MARTIN|SANDRA.MARTIN@sak...|        20|      true| 2020-02-14|2020-02-15 09:57:20|     0|\n",
            "|         17|       1|     DONNA| THOMPSON|DONNA.THOMPSON@sa...|        21|      true| 2020-02-14|2020-02-15 09:57:20|     1|\n",
            "|         18|       2|     CAROL|   GARCIA|CAROL.GARCIA@saki...|        22|      true| 2020-02-14|2020-02-15 09:57:20|     1|\n",
            "|         19|       1|      RUTH| MARTINEZ|RUTH.MARTINEZ@sak...|        23|      true| 2020-02-14|2020-02-15 09:57:20|     1|\n",
            "|         20|       2|    SHARON| ROBINSON|SHARON.ROBINSON@s...|        24|      true| 2020-02-14|2020-02-15 09:57:20|     1|\n",
            "+-----------+--------+----------+---------+--------------------+----------+----------+-----------+-------------------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count = spark.sql(\"\"\"\n",
        "      SELECT DISTINCT email, COUNT(*) as freq\n",
        "      FROM customer\n",
        "      GROUP BY email\n",
        "                           \"\"\")\n",
        "\n",
        "count.registerTempTable(\"count\")\n",
        "\n",
        "sum = spark.sql(\"\"\"\n",
        "      SELECT SUM(freq) as no_of_unique_emails FROM count\n",
        "                           \"\"\")\n",
        "\n",
        "sum.show()"
      ],
      "metadata": {
        "id": "x_ZEWCWV7FKn",
        "outputId": "c97daea1-3537-42fd-a36b-095e1e22df15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+\n",
            "|no_of_unique_emails|\n",
            "+-------------------+\n",
            "|                599|\n",
            "+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 3 \n",
        "\n",
        "Repeat this calculation using only the Dataframe API and print the result."
      ],
      "metadata": {
        "id": "bg7To_5dCRGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pyspark code\n",
        "\n",
        "from pyspark.sql import functions as f\n",
        "\n",
        "count = iris_df.groupBy('email').count()\n",
        "\n",
        "sum = count.groupBy('count').agg(f.sum('count').alias('no_of_unique_emails'))\n",
        "\n",
        "sum.select('no_of_unique_emails').show()"
      ],
      "metadata": {
        "id": "hTO78anmCa37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08f1e1d0-cc27-4000-f7a1-20336419dce3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------+\n",
            "|no_of_unique_emails|\n",
            "+-------------------+\n",
            "|                599|\n",
            "+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 4 \n",
        "\n",
        "How many partitions are present in the dataframe resulting from Question 3 (additionally provide the code necessary to determine that)"
      ],
      "metadata": {
        "id": "8IIL4RDSCcn4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BgImbl6m7Omr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 5\n",
        "\n",
        "Compute the min and max of customer.create_date and print the result (once more using the Spark DataFrame API and not the Spark SQL API)."
      ],
      "metadata": {
        "id": "P_6o4oLIC5SJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#maximum create date\n",
        "iris_df.select('create_date').orderBy('create_date', ascending=False).show(1)\n",
        "\n",
        "#minimum create date\n",
        "iris_df.select('create_date').orderBy('create_date', ascending=True).show(1)"
      ],
      "metadata": {
        "id": "F8XRJZxMG1uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 6.1\n",
        "\n",
        "Determine which first names occur more than once:\n",
        "\n",
        "1. using the Spark SQL API (printing the result)"
      ],
      "metadata": {
        "id": "8vndZmoyC-Ay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count = spark.sql(\"\"\"\n",
        "      SELECT first_name, COUNT(*) as freq\n",
        "      FROM customer\n",
        "      GROUP BY first_name\n",
        "                           \"\"\")\n",
        "\n",
        "count.registerTempTable(\"count\")\n",
        "\n",
        "count2 = spark.sql(\"\"\"\n",
        "      SELECT first_name, freq\n",
        "      FROM count\n",
        "      WHERE freq > 1\n",
        "                           \"\"\")\n",
        "\n",
        "count2.show()\n"
      ],
      "metadata": {
        "id": "SOCV5hTrIqxk",
        "outputId": "54e54c63-6fff-4bff-fb4f-69297232dc69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----+\n",
            "|first_name|freq|\n",
            "+----------+----+\n",
            "|     TERRY|   2|\n",
            "|    WILLIE|   2|\n",
            "|    MARION|   2|\n",
            "|     KELLY|   2|\n",
            "|    LESLIE|   2|\n",
            "|     JAMIE|   2|\n",
            "|     TRACY|   2|\n",
            "|    JESSIE|   2|\n",
            "+----------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 6.2\n",
        "\n",
        "  2. using the Spark Dataframe API (printing the result once more)."
      ],
      "metadata": {
        "id": "d-qGmjBqDErO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count = iris_df.groupBy('first_name').count().orderBy('count', ascending=False).show()"
      ],
      "metadata": {
        "id": "EksZtkgnSARX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 7\n",
        "\n",
        "Port the PostgreSQL below to the PySpark DataFrame API and execute the query within Spark (not directly on PostgreSQL): \n",
        "\n",
        "```\n",
        "SELECT\n",
        "   staff.first_name\n",
        "   ,staff.last_name\n",
        "   ,SUM(payment.amount)\n",
        " FROM payment\n",
        "   INNER JOIN staff ON payment.staff_id = staff.staff_id\n",
        " WHERE payment.payment_date BETWEEN '2007-01-01' AND '2007-02-01'\n",
        " GROUP BY\n",
        "   staff.last_name\n",
        "   ,staff.first_name\n",
        " ORDER BY SUM(payment.amount)\n",
        " ;\n",
        "```"
      ],
      "metadata": {
        "id": "qA56WFXXDqrm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 8\n",
        "\n",
        "Are you currently executing commands on a driver node, or a worker? Provide the code you ran to determine that."
      ],
      "metadata": {
        "id": "Qqv7FoidJiBJ"
      }
    }
  ]
}